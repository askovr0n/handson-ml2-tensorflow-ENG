{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My first neural network implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train a deep multilayer perceptron on the MNIST data set (you can load it using the keras.data sets.mnist.load_data() function). Try to get a precision of more than 98%. Try to find the optimal learning rate using the technique described in this chapter (i.e. exponentially increasing the value of the (i.e. exponentially increasing the value of the learning rate in each iteration, creating a graph of the loss function and finding the  point at which value of this function begins to increase). Try adding other features, such as saving checkpoints, early stopping try adding other features, such as saving control points, stopping early,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dividing the dataset into train, valid and test sample, let's create my first neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = keras.models.Sequential()\n",
    "network.add(keras.layers.Flatten(input_shape = [28, 28]))\n",
    "network.add(keras.layers.Dense(300, activation = 'relu'))\n",
    "network.add(keras.layers.Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(loss = \"sparse_categorical_crossentropy\", \n",
    "                optimizer = \"sgd\", #backpropagation\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                3010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 238,510\n",
      "Trainable params: 238,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.6529 - accuracy: 0.8380 - val_loss: 0.3583 - val_accuracy: 0.9080\n",
      "Epoch 2/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3412 - accuracy: 0.9051 - val_loss: 0.2886 - val_accuracy: 0.9204\n",
      "Epoch 3/20\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2907 - accuracy: 0.9186 - val_loss: 0.2536 - val_accuracy: 0.9314\n",
      "Epoch 4/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2595 - accuracy: 0.9270 - val_loss: 0.2312 - val_accuracy: 0.9368\n",
      "Epoch 5/20\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2362 - accuracy: 0.9338 - val_loss: 0.2122 - val_accuracy: 0.9424\n",
      "Epoch 6/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2174 - accuracy: 0.9394 - val_loss: 0.1987 - val_accuracy: 0.9444\n",
      "Epoch 7/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2014 - accuracy: 0.9443 - val_loss: 0.1842 - val_accuracy: 0.9484\n",
      "Epoch 8/20\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.1877 - accuracy: 0.9471 - val_loss: 0.1748 - val_accuracy: 0.9524\n",
      "Epoch 9/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1756 - accuracy: 0.9510 - val_loss: 0.1647 - val_accuracy: 0.9548\n",
      "Epoch 10/20\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.1652 - accuracy: 0.9535 - val_loss: 0.1552 - val_accuracy: 0.9576\n",
      "Epoch 11/20\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1558 - accuracy: 0.9566 - val_loss: 0.1479 - val_accuracy: 0.9598\n",
      "Epoch 12/20\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.1474 - accuracy: 0.9586 - val_loss: 0.1417 - val_accuracy: 0.9620\n",
      "Epoch 13/20\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.1398 - accuracy: 0.9609 - val_loss: 0.1358 - val_accuracy: 0.9650\n",
      "Epoch 14/20\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1330 - accuracy: 0.9635 - val_loss: 0.1318 - val_accuracy: 0.9650\n",
      "Epoch 15/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1268 - accuracy: 0.9651 - val_loss: 0.1248 - val_accuracy: 0.9666\n",
      "Epoch 16/20\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.1211 - accuracy: 0.9667 - val_loss: 0.1222 - val_accuracy: 0.9684\n",
      "Epoch 17/20\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.1160 - accuracy: 0.9681 - val_loss: 0.1186 - val_accuracy: 0.9686\n",
      "Epoch 18/20\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1112 - accuracy: 0.9696 - val_loss: 0.1152 - val_accuracy: 0.9698\n",
      "Epoch 19/20\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.1067 - accuracy: 0.9710 - val_loss: 0.1121 - val_accuracy: 0.9694\n",
      "Epoch 20/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1025 - accuracy: 0.9719 - val_loss: 0.1100 - val_accuracy: 0.9704\n"
     ]
    }
   ],
   "source": [
    "history = network.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial accuracy, at 20 epochs, is 0.9719. Let's illustrate this in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I learn, I see that the accuracy for my data increased as the loss function decreased. However, it was not possible, in the first iteration, to get an accuracy greater than/equal to 0.98. Now I will play with some parameters and see what happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0690 - accuracy: 0.9789\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06895820051431656, 0.9789000153541565]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10)\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_model_mnist.h5\", save_best_only=True)\n",
    "\n",
    "history_es = network.fit(X_train, y_train, epochs = 50,\n",
    "                      validation_data = (X_valid, y_valid),\n",
    "                      callbacks = [early_stopping_cb, checkpoint_cb],\n",
    "                      verbose = 0 #don't print each epoch\n",
    "                      )\n",
    "                      \n",
    "best_model = keras.models.load_model(\"my_model_mnist.h5\") # Come back to the best model\n",
    "best_model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the best model, on my test dataset I reached a score higher than 0.98. Nevertheless, let's check also for the training data, how many models were above accuracy 0.98."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sum of epochs were accuracy was higher than 0.98 is 41.\n"
     ]
    }
   ],
   "source": [
    "result = 0\n",
    "\n",
    "for i in history_es.history[\"accuracy\"]:\n",
    "\n",
    "    if i > 0.98:\n",
    "        result+=1\n",
    "\n",
    "print(f'Total sum of epochs were accuracy was higher than 0.98 is {result}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the 50 epochs, 13 of them had an accuracy greater than 0.98. Thus, the assumption from the task was met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am compiling my neural network for learning rate = 0.005\n",
      "1719/1719 [==============================] - 5s 2ms/step - loss: 0.0280 - accuracy: 0.9946 - val_loss: 0.0644 - val_accuracy: 0.9814\n",
      "Evaluation for test dataset\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0685 - accuracy: 0.9789\n",
      "------------------------------End of compilation for learning rate = 0.005\n",
      "I am compiling my neural network for learning rate = 0.01\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0282 - accuracy: 0.9944 - val_loss: 0.0645 - val_accuracy: 0.9810\n",
      "Evaluation for test dataset\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0684 - accuracy: 0.9787\n",
      "------------------------------End of compilation for learning rate = 0.01\n",
      "I am compiling my neural network for learning rate = 0.1\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0417 - accuracy: 0.9877 - val_loss: 0.0710 - val_accuracy: 0.9774\n",
      "Evaluation for test dataset\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0735 - accuracy: 0.9765\n",
      "------------------------------End of compilation for learning rate = 0.1\n"
     ]
    }
   ],
   "source": [
    "for lr in [0.005, 0.010, 0.1]:\n",
    "\n",
    "    print(f'I am compiling my neural network for learning rate = {lr}')\n",
    "\n",
    "    network.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "                    optimizer=keras.optimizers.SGD(learning_rate=lr),\n",
    "                    metrics=[\"accuracy\"])\n",
    "\n",
    "    history_lr = network.fit(X_train, y_train, epochs=1,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "    \n",
    "    print(\"Evaluation for test dataset\")\n",
    "    network.evaluate(X_test, y_test)\n",
    "\n",
    "    print(\"-\"*30, end = \"\")\n",
    "    print(f'End of compilation for learning rate = {lr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When changing the learning rate, accuracy of over 98% was also achieved. However, the best effect was obtained when oscillating around 5%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning of hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a new neural network in which I will gain more control of some basic hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_scikit_style(n_hidden=1, n_neurons=300, learning_rate=0.01, input_shape = [28,28]):\n",
    "    neural_network_new = keras.models.Sequential()\n",
    "    neural_network_new.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden): # basic NN with one hidden layout\n",
    "        neural_network_new.add(keras.layers.Dense(n_neurons))\n",
    "    neural_network_new.add(keras.layers.Dense(1, activation = 'softmax'))\n",
    "    optimizer = keras.optimizers.SGD(learning_rate = learning_rate)\n",
    "    neural_network_new.compile(loss = \"mse\", optimizer=optimizer)\n",
    "    return neural_network_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-b947a57c0e1e>:2: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = keras.wrappers.scikit_learn.KerasRegressor(neural_network_scikit_style)\n"
     ]
    }
   ],
   "source": [
    "# Indication to scikit learn\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(neural_network_scikit_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 20.2782 - val_loss: 20.2682\n",
      "Epoch 2/50\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 20.2782 - val_loss: 20.2682\n",
      "Epoch 3/50\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 20.2782 - val_loss: 20.2682\n",
      "Epoch 4/50\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 20.2782 - val_loss: 20.2682\n",
      "Epoch 5/50\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 20.2782 - val_loss: 20.2682\n",
      "Epoch 6/50\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 20.2782 - val_loss: 20.2682\n",
      "Epoch 7/50\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 20.2782 - val_loss: 20.2682\n",
      "Epoch 8/50\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 20.2782 - val_loss: 20.2682\n",
      "Epoch 9/50\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 20.2782 - val_loss: 20.2682\n",
      "Epoch 10/50\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 20.2782 - val_loss: 20.2682\n",
      "Epoch 11/50\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 20.2782 - val_loss: 20.2682\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 20.2422\n"
     ]
    }
   ],
   "source": [
    "keras_reg.fit(X_train, y_train, epochs=50,\n",
    "                                validation_data=(X_valid, y_valid),\n",
    "                                callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "scc_test = keras_reg.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1146/1146 [==============================] - 5s 4ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 2/30\n",
      "1146/1146 [==============================] - 3s 3ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 3/30\n",
      "1146/1146 [==============================] - 3s 3ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 4/30\n",
      "1146/1146 [==============================] - 3s 3ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 5/30\n",
      "1146/1146 [==============================] - 3s 3ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 6/30\n",
      "1146/1146 [==============================] - 3s 3ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "573/573 [==============================] - 1s 2ms/step - loss: 20.2081\n",
      "Epoch 1/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 2/30\n",
      "1146/1146 [==============================] - 3s 3ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 3/30\n",
      "1146/1146 [==============================] - 3s 3ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 4/30\n",
      "1146/1146 [==============================] - 3s 3ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 5/30\n",
      "1146/1146 [==============================] - 3s 3ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 6/30\n",
      "1146/1146 [==============================] - 3s 3ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "573/573 [==============================] - 1s 2ms/step - loss: 20.3110\n",
      "Epoch 1/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 2/30\n",
      "1146/1146 [==============================] - 3s 3ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 3/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 4/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 5/30\n",
      "1146/1146 [==============================] - 3s 3ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 6/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "573/573 [==============================] - 1s 2ms/step - loss: 20.3156\n",
      "Epoch 1/30\n",
      "1146/1146 [==============================] - 3s 3ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 2/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 3/30\n",
      "1146/1146 [==============================] - 3s 3ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 4/30\n",
      "1146/1146 [==============================] - 3s 3ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 5/30\n",
      "1146/1146 [==============================] - 4s 4ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 6/30\n",
      "1146/1146 [==============================] - 3s 3ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "573/573 [==============================] - 1s 2ms/step - loss: 20.2081\n",
      "Epoch 1/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 2/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 3/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 4/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 5/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 6/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "573/573 [==============================] - 1s 2ms/step - loss: 20.3110\n",
      "Epoch 1/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 2/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 3/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 4/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 5/30\n",
      "1146/1146 [==============================] - 2s 2ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 6/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "573/573 [==============================] - 1s 2ms/step - loss: 20.3156\n",
      "Epoch 1/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 2/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 3/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 4/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 5/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 6/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "573/573 [==============================] - 1s 2ms/step - loss: 20.2081\n",
      "Epoch 1/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 2/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 3/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 4/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 5/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 6/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "573/573 [==============================] - 1s 2ms/step - loss: 20.3110\n",
      "Epoch 1/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 2/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 3/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 4/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 5/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 6/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "573/573 [==============================] - 1s 2ms/step - loss: 20.3156\n",
      "Epoch 1/30\n",
      "1146/1146 [==============================] - 5s 4ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 2/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 3/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 4/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 5/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 6/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "573/573 [==============================] - 1s 2ms/step - loss: 20.2081\n",
      "Epoch 1/30\n",
      "1146/1146 [==============================] - 5s 3ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 2/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 3/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 4/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 5/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 6/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "573/573 [==============================] - 1s 2ms/step - loss: 20.3110\n",
      "Epoch 1/30\n",
      "1146/1146 [==============================] - 5s 4ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 2/30\n",
      "1146/1146 [==============================] - 4s 4ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 3/30\n",
      "1146/1146 [==============================] - 5s 4ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 4/30\n",
      "1146/1146 [==============================] - 4s 4ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 5/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 6/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "573/573 [==============================] - 1s 2ms/step - loss: 20.3156\n",
      "Epoch 1/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 2/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 3/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 4/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 5/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 6/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "573/573 [==============================] - 1s 2ms/step - loss: 20.2081\n",
      "Epoch 1/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 2/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 3/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 4/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 5/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 6/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "573/573 [==============================] - 1s 2ms/step - loss: 20.3110\n",
      "Epoch 1/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 2/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 3/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 4/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 5/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 6/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "573/573 [==============================] - 1s 2ms/step - loss: 20.3156\n",
      "Epoch 1/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 2/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 3/30\n",
      "1146/1146 [==============================] - 3s 3ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 4/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 5/30\n",
      "1146/1146 [==============================] - 4s 4ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 6/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "573/573 [==============================] - 1s 2ms/step - loss: 20.2081\n",
      "Epoch 1/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 2/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 3/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 4/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 5/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 6/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "573/573 [==============================] - 1s 2ms/step - loss: 20.3110\n",
      "Epoch 1/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 2/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 3/30\n",
      "1146/1146 [==============================] - 4s 4ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 4/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 5/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 6/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "573/573 [==============================] - 1s 2ms/step - loss: 20.3156\n",
      "Epoch 1/30\n",
      "1146/1146 [==============================] - 6s 5ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 2/30\n",
      "1146/1146 [==============================] - 5s 4ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 3/30\n",
      "1146/1146 [==============================] - 5s 4ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 4/30\n",
      "1146/1146 [==============================] - 5s 4ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 5/30\n",
      "1146/1146 [==============================] - 5s 4ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 6/30\n",
      "1146/1146 [==============================] - 5s 4ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "573/573 [==============================] - 1s 2ms/step - loss: 20.2081\n",
      "Epoch 1/30\n",
      "1146/1146 [==============================] - 6s 4ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 2/30\n",
      "1146/1146 [==============================] - 5s 5ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 3/30\n",
      "1146/1146 [==============================] - 5s 4ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 4/30\n",
      "1146/1146 [==============================] - 6s 5ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 5/30\n",
      "1146/1146 [==============================] - 6s 5ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 6/30\n",
      "1146/1146 [==============================] - 5s 4ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "573/573 [==============================] - 1s 2ms/step - loss: 20.3110\n",
      "Epoch 1/30\n",
      "1146/1146 [==============================] - 6s 4ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 2/30\n",
      "1146/1146 [==============================] - 5s 4ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 3/30\n",
      "1146/1146 [==============================] - 5s 4ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 4/30\n",
      "1146/1146 [==============================] - 5s 4ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 5/30\n",
      "1146/1146 [==============================] - 5s 4ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 6/30\n",
      "1146/1146 [==============================] - 5s 5ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "573/573 [==============================] - 2s 3ms/step - loss: 20.3156\n",
      "Epoch 1/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 2/30\n",
      "1146/1146 [==============================] - 3s 3ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 3/30\n",
      "1146/1146 [==============================] - 3s 3ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 4/30\n",
      "1146/1146 [==============================] - 3s 3ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 5/30\n",
      "1146/1146 [==============================] - 3s 3ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 6/30\n",
      "1146/1146 [==============================] - 3s 3ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "573/573 [==============================] - 1s 2ms/step - loss: 20.2081\n",
      "Epoch 1/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 2/30\n",
      "1146/1146 [==============================] - 3s 3ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 3/30\n",
      "1146/1146 [==============================] - 3s 3ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 4/30\n",
      "1146/1146 [==============================] - 3s 3ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 5/30\n",
      "1146/1146 [==============================] - 3s 3ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 6/30\n",
      "1146/1146 [==============================] - 3s 3ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "573/573 [==============================] - 1s 2ms/step - loss: 20.3110\n",
      "Epoch 1/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 2/30\n",
      "1146/1146 [==============================] - 3s 3ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 3/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 4/30\n",
      "1146/1146 [==============================] - 3s 3ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 5/30\n",
      "1146/1146 [==============================] - 3s 3ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 6/30\n",
      "1146/1146 [==============================] - 3s 3ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "573/573 [==============================] - 1s 2ms/step - loss: 20.3156\n",
      "Epoch 1/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 2/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 3/30\n",
      "1146/1146 [==============================] - 4s 3ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 4/30\n",
      "1146/1146 [==============================] - 3s 3ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 5/30\n",
      "1146/1146 [==============================] - 3s 3ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 6/30\n",
      "1146/1146 [==============================] - 5s 4ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "573/573 [==============================] - 2s 3ms/step - loss: 20.2081\n",
      "Epoch 1/30\n",
      "1146/1146 [==============================] - 7s 5ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 2/30\n",
      "1146/1146 [==============================] - 4s 4ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 3/30\n",
      "1146/1146 [==============================] - 4s 4ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 4/30\n",
      "1146/1146 [==============================] - 4s 4ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 5/30\n",
      "1146/1146 [==============================] - 4s 4ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 6/30\n",
      "1146/1146 [==============================] - 4s 4ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "573/573 [==============================] - 1s 2ms/step - loss: 20.3110\n",
      "Epoch 1/30\n",
      "1146/1146 [==============================] - 5s 4ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 2/30\n",
      "1146/1146 [==============================] - 4s 4ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 3/30\n",
      "1146/1146 [==============================] - 4s 4ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 4/30\n",
      "1146/1146 [==============================] - 4s 4ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 5/30\n",
      "1146/1146 [==============================] - 4s 4ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 6/30\n",
      "1146/1146 [==============================] - 4s 4ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "573/573 [==============================] - 1s 2ms/step - loss: 20.3156\n",
      "Epoch 1/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 2/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 3/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 4/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 5/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "Epoch 6/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.3133 - val_loss: 20.2682\n",
      "573/573 [==============================] - 1s 2ms/step - loss: 20.2081\n",
      "Epoch 1/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 2/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 3/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 4/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 5/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "Epoch 6/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2618 - val_loss: 20.2682\n",
      "573/573 [==============================] - 1s 1ms/step - loss: 20.3110\n",
      "Epoch 1/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 2/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 3/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 4/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 5/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "Epoch 6/30\n",
      "1146/1146 [==============================] - 3s 2ms/step - loss: 20.2596 - val_loss: 20.2682\n",
      "573/573 [==============================] - 1s 2ms/step - loss: 20.3156\n",
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 20.2782 - val_loss: 20.2682\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 20.2782 - val_loss: 20.2682\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 20.2782 - val_loss: 20.2682\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 20.2782 - val_loss: 20.2682\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 20.2782 - val_loss: 20.2682\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 20.2782 - val_loss: 20.2682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x000001F6750CEF70>,\n",
       "                   param_distributions={'learning_rate': [0.0001, 0.01, 0.1],\n",
       "                                        'n_hidden': [1, 2, 3],\n",
       "                                        'n_neurons': [20, 30, 50, 100]})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tuning\n",
    "param_distribs = {\n",
    " \"n_hidden\": [1, 2, 3],\n",
    " \"n_neurons\": [20,30,50,100],\n",
    " \"learning_rate\": [0.0001, 0.01, 0.10],\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=30,\n",
    "                                    validation_data=(X_valid, y_valid),\n",
    "                                    callbacks=[keras.callbacks.EarlyStopping(patience=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neurons': 100, 'n_hidden': 1, 'learning_rate': 0.01}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-20.27823766072591"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "58c6268f8c0b696f0c5de63073949a37c81bc2ff28536b69e37f15be4ce332ad"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
